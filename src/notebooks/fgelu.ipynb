{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author: Simo Ryu\n",
    "tag: research\n",
    "title: Is that a faster GELU?\n",
    "abstract: We have a deeper look at recent post by Daniel de Kok in twitter (https://twitter.com/danieldekok/status/1484898130441166853). Is that function a faster GELU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent twitter post by @danieldekok (https://twitter.com/danieldekok/status/1484898130441166853) mentions that there is a faster version of the GELU activation function. He mentions: \n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Did anyone try this activation function? It is very similar to GELU and Swish, but has the benefit that it can be implemented easily using SIMD on modern CPUs. <a href=\"https://t.co/4WytEyhueD\">pic.twitter.com/4WytEyhueD</a></p>&mdash; DaniÃ«l de Kok ðŸ’‰ðŸ’‰ (@danieldekok) <a href=\"https://twitter.com/danieldekok/status/1484898130441166853?ref_src=twsrc%5Etfw\">January 22, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function looks like this: \n",
    "\n",
    "$$\n",
    "y = 0.5 x (1 + \\frac{x}{\\sqrt{1 + x^2}})\n",
    "$$\n",
    "\n",
    "We will compare the performance and efficiency of GELU and the faster version of GELU. From now on, the faster GELU will be named DaniGELU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the DaniGELU function faster than GELU? Let's compare it with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13763469457626343\n",
      "-0.12556812167167664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# we would normally define it as follows:\n",
    "\n",
    "class DaniGELU(nn.Module):\n",
    "    '''\n",
    "    Activation such that:\n",
    "    $$\n",
    "    y = 0.5 x (1 + \\frac{x}{\\sqrt{1 + x^2}})\n",
    "    $$\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DaniGELU, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + x / torch.sqrt(1 + x**2))\n",
    "\n",
    "\n",
    "gelu = nn.GELU()\n",
    "dani_gelu = DaniGELU()\n",
    "\n",
    "# test\n",
    "x = torch.randn(1, 1, 1, 1)\n",
    "print(gelu(x).item())\n",
    "print(dani_gelu(x).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dani_gelu time: 0.12120413780212402\n",
      "gelu time: 0.017823219299316406\n"
     ]
    }
   ],
   "source": [
    "#rm\n",
    "import time\n",
    "\n",
    "# test dain_gelu time performance\n",
    "start = time.time()\n",
    "for i in range(10000):\n",
    "    dani_gelu(x)\n",
    "end = time.time()\n",
    "\n",
    "print('dani_gelu time:', end - start)\n",
    "\n",
    "# test gelu time performance\n",
    "\n",
    "start = time.time()\n",
    "for i in range(10000):\n",
    "    gelu(x)\n",
    "end = time.time()\n",
    "\n",
    "print('gelu time:', end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I doubt that the difference is caused by the actual logic of the function. Clearly, difference is caused by the implementation of the function.\n",
    "Inside, GeLU is implemented with c++ backend, and the faster version is implemented with cuda backend. However, DaniGELU above is implemented with python.\n",
    "\n",
    "Fair comparison would be to implment the function with c++, and compare the performance. Let's have a look at just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <torch/script.h>\n",
    "\n",
    "\n",
    "torch::Tensor GELU(torch::Tensor x) {\n",
    "    return 0.5 * x * (1 + torch::erf(x / torch::sqrt(2)));\n",
    "}\n",
    "\n",
    "\n",
    "torch::Tensor daniGELU(torch::Tensor x) {\n",
    "  return 0.5 * x * (1 + torch::sqrt(1 + x.pow(2)));\n",
    "}\n",
    "\n",
    "\n",
    "int main() {\n",
    "  torch::DeviceType device_type;\n",
    "  if (torch::cuda::is_available()) {\n",
    "    device_type = torch::kCUDA;\n",
    "  } else {\n",
    "    device_type = torch::kCPU;\n",
    "  }\n",
    "  torch::Device device(device_type);\n",
    "\n",
    "  torch::Tensor x = torch::randn({1, 1}, device);\n",
    "  torch::Tensor geLU_out = GELU(x);\n",
    "  torch::Tensor daniGELU_out = daniGELU(x);\n",
    "\n",
    "  std::cout << \"GELU: \" << geLU_out << std::endl;\n",
    "  std::cout << \"daniGELU: \" << daniGELU_out << std::endl;\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "952e1bebe1b278d85469a034aefc1854b777c1b518feedf8249123f6f86cec05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_latest': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
